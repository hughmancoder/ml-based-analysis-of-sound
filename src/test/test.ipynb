{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3920cdbd",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "# Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6dc9ec36",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "while ROOT != ROOT.parent and not (ROOT / \"src\").exists():\n",
        "    ROOT = ROOT.parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from src.models.CNN import CNN\n",
        "from src.test.utils import  display_formatted_results, run_inference\n",
        "\n",
        "MODEL_WEIGHTS     = ROOT / \"src/models/saved_weights/CNN_v0/best_micro_f1.pt\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea3a78b",
      "metadata": {},
      "source": [
        "## Run on A Touch of Zen Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0cbd003d",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/hughsignoriello/Developer/ml-based-analysis-of-sound/src/models/saved_weights/CNN_v0/best_micro_f1.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m test_manifest_csv = ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata/test/a-touch-of-zen.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m preds_arr, gts_arr, sample_ids, audio_cfg, valid_labels, label_to_idx = \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min_ch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_weights_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_WEIGHTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_manifest_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_manifest_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/ml-based-analysis-of-sound/src/test/utils.py:154\u001b[39m, in \u001b[36mrun_inference\u001b[39m\u001b[34m(model_cls, model_kwargs, model_weights_path, device, test_manifest_csv, root, state_key, audio_cfg_key, classes_key, strict_load, show_progress)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_inference\u001b[39m(\n\u001b[32m    130\u001b[39m     *,\n\u001b[32m    131\u001b[39m     model_cls,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m     show_progress: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    142\u001b[39m ):\n\u001b[32m    143\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m    Loads checkpoint + model, runs inference over a manifest CSV.\u001b[39;00m\n\u001b[32m    145\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m \u001b[33;03m        label_to_idx: dict[str, int]\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     audio_cfg = ckpt[audio_cfg_key]\n\u001b[32m    157\u001b[39m     valid_labels = [c.strip().lower() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ckpt[classes_key]]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/ml-based-analysis-of-sound/.venv/lib/python3.13/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/ml-based-analysis-of-sound/.venv/lib/python3.13/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/ml-based-analysis-of-sound/.venv/lib/python3.13/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/hughsignoriello/Developer/ml-based-analysis-of-sound/src/models/saved_weights/CNN_v0/best_micro_f1.pt'"
          ]
        }
      ],
      "source": [
        "test_manifest_csv = ROOT / \"data/test/a-touch-of-zen.csv\"\n",
        "\n",
        "preds_arr, gts_arr, sample_ids, audio_cfg, valid_labels, label_to_idx = run_inference(\n",
        "    model_cls=CNN,\n",
        "    model_kwargs={\"in_ch\": 2},\n",
        "    model_weights_path=MODEL_WEIGHTS,\n",
        "    device=DEVICE,\n",
        "    test_manifest_csv=test_manifest_csv,\n",
        "    root=ROOT,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c4507a",
      "metadata": {},
      "source": [
        "## Tune classification threshold probability "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22e85ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold  | Micro F1   | Macro F1   | Subset Acc\n",
            "--------------------------------------------------\n",
            "0.05       | 0.1633     | 0.1095     | 2.35%     \n",
            "0.10       | 0.1413     | 0.0877     | 3.53%     \n",
            "0.15       | 0.1339     | 0.0838     | 4.71%     \n",
            "0.20       | 0.1333     | 0.0876     | 5.88%     \n",
            "0.25       | 0.1327     | 0.0865     | 7.06%     \n",
            "0.30       | 0.1116     | 0.0720     | 7.06%     \n",
            "0.35       | 0.1053     | 0.0717     | 7.06%     \n",
            "0.40       | 0.0891     | 0.0677     | 7.06%     \n",
            "0.45       | 0.0800     | 0.0515     | 7.06%     \n",
            "0.50       | 0.0804     | 0.0518     | 7.06%     \n",
            "0.55       | 0.0513     | 0.0244     | 5.88%     \n",
            "0.60       | 0.0532     | 0.0256     | 8.24%     \n",
            "0.65       | 0.0535     | 0.0256     | 8.24%     \n",
            "0.70       | 0.0543     | 0.0269     | 10.59%    \n",
            "0.75       | 0.0556     | 0.0283     | 10.59%    \n",
            "0.80       | 0.0565     | 0.0292     | 10.59%    \n",
            "0.85       | 0.0465     | 0.0263     | 11.76%    \n",
            "0.90       | 0.0357     | 0.0160     | 11.76%    \n",
            "\n",
            "Best Threshold (Micro F1): 0.05 (Score: 0.1633)\n",
            "Best Threshold (Macro F1): 0.05 (Score: 0.1095)\n"
          ]
        }
      ],
      "source": [
        "from src.test.utils import find_best_threshold\n",
        "best_t = find_best_threshold(preds_arr, gts_arr, valid_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca33bfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification threshold probability: 0.5\n",
            "Total Samples: 85\n",
            "Predicted 'None' (all-zero): 38  (44.71%)\n",
            "\n",
            "Hamming accuracy: 80.43%\n",
            "Subset accuracy (exact match): 7.06%\n",
            "Micro-Average F1: 0.0804\n",
            "Macro-Average F1: 0.0518\n",
            "\n",
            "Instrument      |   Prec | Recall |     F1 | Support\n",
            "-------------------------------------------------------\n",
            "pipa            |   0.00 |   0.00 |   0.00 |      29\n",
            "erhu            |   0.00 |   0.00 |   0.00 |      23\n",
            "sheng           |   0.00 |   0.00 |   0.00 |      30\n",
            "dizi            |   0.00 |   0.00 |   0.00 |      12\n",
            "xiao            |   0.00 |   0.00 |   0.00 |       8\n",
            "piano           |   0.00 |   0.00 |   0.00 |       2\n",
            "guqin           |   0.00 |   0.00 |   0.00 |      10\n",
            "suona           |   0.20 |   0.25 |   0.22 |       4\n",
            "guzheng         |   0.00 |   0.00 |   0.00 |       2\n",
            "percussion      |   0.32 |   0.22 |   0.26 |      27\n",
            "voice           |   0.05 |   0.25 |   0.09 |       4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, hamming_loss\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, hamming_loss\n",
        "\n",
        "\n",
        "def evaluate_multilabel_performance(\n",
        "    all_preds,\n",
        "    all_gt,\n",
        "    class_list,\n",
        "    sample_ids=None,\n",
        "    threshold=0.5,\n",
        "    debug=False,\n",
        "    zero_division=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Multi-label evaluation with sanity checks and sensible per-class metrics.\n",
        "    Note: If ground-truth has no positives, metrics are meaningless and we warn loudly.\n",
        "    \"\"\"\n",
        "    # ---- 1) Setup ----\n",
        "    classes = [c.strip().lower() for c in class_list]\n",
        "    probs = np.asarray(all_preds)\n",
        "    gts = np.asarray(all_gt).astype(int)\n",
        "\n",
        "    if probs.shape != gts.shape:\n",
        "        raise ValueError(f\"Shape mismatch: preds {probs.shape} vs gts {gts.shape}\")\n",
        "\n",
        "    # ---- 2) Threshold ----\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    # ---- 3) Sanity checks ----\n",
        "    total_pos = int(gts.sum())\n",
        "    pos_per_class = gts.sum(axis=0)\n",
        "\n",
        "    if total_pos == 0:\n",
        "        print(\"WARNING: Ground-truth contains ZERO positive labels across all samples.\")\n",
        "        print(\"This usually means your test labels are not being mapped into label_to_idx\")\n",
        "        print(\"(e.g. label mismatch or parse_ground_truth drops unknown labels).\")\n",
        "        print(\"Any 'accuracy' you see will be dominated by true negatives and is not meaningful.\\n\")\n",
        "\n",
        "    # ---- 4) 'None' prediction rate (informational only) ----\n",
        "    none_pred_mask = preds.sum(axis=1) == 0\n",
        "    num_total = preds.shape[0]\n",
        "    num_none = int(none_pred_mask.sum())\n",
        "    num_non_none = num_total - num_none\n",
        "\n",
        "    # ---- 5) Global metrics on FULL set ----\n",
        "    exact_match = accuracy_score(gts, preds)                 # subset accuracy\n",
        "    hacc = 1.0 - hamming_loss(gts, preds)                    # hamming \"accuracy\"\n",
        "\n",
        "    report = classification_report(\n",
        "        gts,\n",
        "        preds,\n",
        "        target_names=classes,\n",
        "        output_dict=True,\n",
        "        zero_division=zero_division,\n",
        "    )\n",
        "\n",
        "    print(f\"Classification threshold probability: {threshold}\")\n",
        "    print(f\"Total Samples: {num_total}\")\n",
        "    print(f\"Predicted 'None' (all-zero): {num_none}  ({(num_none/num_total):.2%})\")\n",
        "    print(\"\")\n",
        "    print(f\"Hamming accuracy: {hacc:.2%}\")\n",
        "    print(f\"Subset accuracy (exact match): {exact_match:.2%}\")\n",
        "    print(f\"Micro-Average F1: {report['micro avg']['f1-score']:.4f}\")\n",
        "    print(f\"Macro-Average F1: {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(\"\")\n",
        "\n",
        "    # ---- 6) Per-class: Precision / Recall / F1 / Support ----\n",
        "    # (Recall is what will go to 0 if the class appears but you never predict it.)\n",
        "    print(f\"{'Instrument':<15} | {'Prec':>6} | {'Recall':>6} | {'F1':>6} | {'Support':>7}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    for i, name in enumerate(classes):\n",
        "        support = int(pos_per_class[i])\n",
        "\n",
        "        if support == 0:\n",
        "            # No positives in GT for this class -> can't interpret metrics\n",
        "            print(f\"{name:<15} | {'  n/a':>6} | {'  n/a':>6} | {'  n/a':>6} | {support:>7}\")\n",
        "            continue\n",
        "\n",
        "        prec = report[name][\"precision\"]\n",
        "        rec = report[name][\"recall\"]\n",
        "        f1 = report[name][\"f1-score\"]\n",
        "        print(f\"{name:<15} | {prec:6.2f} | {rec:6.2f} | {f1:6.2f} | {support:>7}\")\n",
        "\n",
        "    # ---- 7) Optional debug: show some examples where GT had positives ----\n",
        "    if debug and sample_ids is not None:\n",
        "        print(\"\\n--- DEBUG: Examples where GT has at least one label ---\")\n",
        "        gt_nonzero = np.where(gts.sum(axis=1) > 0)[0]\n",
        "        for idx in gt_nonzero[:20]:\n",
        "            pred_names = [classes[j] for j, v in enumerate(preds[idx]) if v]\n",
        "            gt_names = [classes[j] for j, v in enumerate(gts[idx]) if v]\n",
        "            print(f\"ID: {sample_ids[idx]}\")\n",
        "            print(f\"  Predicted: {pred_names if pred_names else '(none)'}\")\n",
        "            print(f\"  Actual:    {gt_names if gt_names else '(none)'}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "    return report\n",
        "\n",
        "# Default is 0.5; could optionally adjust based on precision-recall tradeoff desired\n",
        "threshold_probability = 0.5\n",
        "\n",
        "# Then, call the evaluation function using that best threshold\n",
        "results = evaluate_multilabel_performance(\n",
        "    all_preds=preds_arr, \n",
        "    all_gt=gts_arr, \n",
        "    class_list=valid_labels, \n",
        "    threshold=threshold_probability,\n",
        "    debug=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253b7212",
      "metadata": {},
      "source": [
        "### Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ef52f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Detailed Classification Report ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_f43b6_row0_col2, #T_f43b6_row1_col2, #T_f43b6_row2_col2, #T_f43b6_row3_col2, #T_f43b6_row4_col2, #T_f43b6_row5_col2, #T_f43b6_row6_col2, #T_f43b6_row8_col2 {\n",
              "  background-color: #ffffd9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f43b6_row7_col2 {\n",
              "  background-color: #253b97;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_f43b6_row9_col2 {\n",
              "  background-color: #081d58;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_f43b6_row10_col2 {\n",
              "  background-color: #97d6b9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f43b6_row11_col2 {\n",
              "  background-color: #a7dcb7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f43b6_row12_col2 {\n",
              "  background-color: #d7efb3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f43b6_row13_col2 {\n",
              "  background-color: #d4eeb3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f43b6_row14_col2 {\n",
              "  background-color: #d5efb3;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_f43b6\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_f43b6_level0_col0\" class=\"col_heading level0 col0\" >precision</th>\n",
              "      <th id=\"T_f43b6_level0_col1\" class=\"col_heading level0 col1\" >recall</th>\n",
              "      <th id=\"T_f43b6_level0_col2\" class=\"col_heading level0 col2\" >f1-score</th>\n",
              "      <th id=\"T_f43b6_level0_col3\" class=\"col_heading level0 col3\" >support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row0\" class=\"row_heading level0 row0\" >pipa</th>\n",
              "      <td id=\"T_f43b6_row0_col0\" class=\"data row0 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row0_col3\" class=\"data row0 col3\" >29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row1\" class=\"row_heading level0 row1\" >erhu</th>\n",
              "      <td id=\"T_f43b6_row1_col0\" class=\"data row1 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row1_col3\" class=\"data row1 col3\" >23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row2\" class=\"row_heading level0 row2\" >sheng</th>\n",
              "      <td id=\"T_f43b6_row2_col0\" class=\"data row2 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row2_col3\" class=\"data row2 col3\" >30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row3\" class=\"row_heading level0 row3\" >dizi</th>\n",
              "      <td id=\"T_f43b6_row3_col0\" class=\"data row3 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row3_col3\" class=\"data row3 col3\" >12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row4\" class=\"row_heading level0 row4\" >xiao</th>\n",
              "      <td id=\"T_f43b6_row4_col0\" class=\"data row4 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row4_col2\" class=\"data row4 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row4_col3\" class=\"data row4 col3\" >8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row5\" class=\"row_heading level0 row5\" >piano</th>\n",
              "      <td id=\"T_f43b6_row5_col0\" class=\"data row5 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row5_col1\" class=\"data row5 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row5_col3\" class=\"data row5 col3\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row6\" class=\"row_heading level0 row6\" >guqin</th>\n",
              "      <td id=\"T_f43b6_row6_col0\" class=\"data row6 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row6_col1\" class=\"data row6 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row6_col2\" class=\"data row6 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row6_col3\" class=\"data row6 col3\" >10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row7\" class=\"row_heading level0 row7\" >suona</th>\n",
              "      <td id=\"T_f43b6_row7_col0\" class=\"data row7 col0\" >0.2000</td>\n",
              "      <td id=\"T_f43b6_row7_col1\" class=\"data row7 col1\" >0.2500</td>\n",
              "      <td id=\"T_f43b6_row7_col2\" class=\"data row7 col2\" >0.2222</td>\n",
              "      <td id=\"T_f43b6_row7_col3\" class=\"data row7 col3\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row8\" class=\"row_heading level0 row8\" >guzheng</th>\n",
              "      <td id=\"T_f43b6_row8_col0\" class=\"data row8 col0\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row8_col1\" class=\"data row8 col1\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row8_col2\" class=\"data row8 col2\" >0.0000</td>\n",
              "      <td id=\"T_f43b6_row8_col3\" class=\"data row8 col3\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row9\" class=\"row_heading level0 row9\" >percussion</th>\n",
              "      <td id=\"T_f43b6_row9_col0\" class=\"data row9 col0\" >0.3158</td>\n",
              "      <td id=\"T_f43b6_row9_col1\" class=\"data row9 col1\" >0.2222</td>\n",
              "      <td id=\"T_f43b6_row9_col2\" class=\"data row9 col2\" >0.2609</td>\n",
              "      <td id=\"T_f43b6_row9_col3\" class=\"data row9 col3\" >27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row10\" class=\"row_heading level0 row10\" >voice</th>\n",
              "      <td id=\"T_f43b6_row10_col0\" class=\"data row10 col0\" >0.0526</td>\n",
              "      <td id=\"T_f43b6_row10_col1\" class=\"data row10 col1\" >0.2500</td>\n",
              "      <td id=\"T_f43b6_row10_col2\" class=\"data row10 col2\" >0.0870</td>\n",
              "      <td id=\"T_f43b6_row10_col3\" class=\"data row10 col3\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row11\" class=\"row_heading level0 row11\" >micro avg</th>\n",
              "      <td id=\"T_f43b6_row11_col0\" class=\"data row11 col0\" >0.1667</td>\n",
              "      <td id=\"T_f43b6_row11_col1\" class=\"data row11 col1\" >0.0530</td>\n",
              "      <td id=\"T_f43b6_row11_col2\" class=\"data row11 col2\" >0.0804</td>\n",
              "      <td id=\"T_f43b6_row11_col3\" class=\"data row11 col3\" >151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row12\" class=\"row_heading level0 row12\" >macro avg</th>\n",
              "      <td id=\"T_f43b6_row12_col0\" class=\"data row12 col0\" >0.0517</td>\n",
              "      <td id=\"T_f43b6_row12_col1\" class=\"data row12 col1\" >0.0657</td>\n",
              "      <td id=\"T_f43b6_row12_col2\" class=\"data row12 col2\" >0.0518</td>\n",
              "      <td id=\"T_f43b6_row12_col3\" class=\"data row12 col3\" >151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row13\" class=\"row_heading level0 row13\" >weighted avg</th>\n",
              "      <td id=\"T_f43b6_row13_col0\" class=\"data row13 col0\" >0.0632</td>\n",
              "      <td id=\"T_f43b6_row13_col1\" class=\"data row13 col1\" >0.0530</td>\n",
              "      <td id=\"T_f43b6_row13_col2\" class=\"data row13 col2\" >0.0548</td>\n",
              "      <td id=\"T_f43b6_row13_col3\" class=\"data row13 col3\" >151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f43b6_level0_row14\" class=\"row_heading level0 row14\" >samples avg</th>\n",
              "      <td id=\"T_f43b6_row14_col0\" class=\"data row14 col0\" >0.0882</td>\n",
              "      <td id=\"T_f43b6_row14_col1\" class=\"data row14 col1\" >0.0429</td>\n",
              "      <td id=\"T_f43b6_row14_col2\" class=\"data row14 col2\" >0.0532</td>\n",
              "      <td id=\"T_f43b6_row14_col3\" class=\"data row14 col3\" >151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x12e515fd0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display_formatted_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9636640",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ckpt = torch.load(MODEL_WEIGHTS, map_location=DEVICE)\n",
        "# audio_cfg = ckpt[\"audio_config\"]\n",
        "# valid_labels = [c.strip().lower() for c in ckpt[\"classes\"]]\n",
        "# label_to_idx = {name: i for i, name in enumerate(valid_labels)}\n",
        "# print(audio_cfg)\n",
        "\n",
        "# model = CNN(in_ch=2, num_classes=len(valid_labels)).to(DEVICE)\n",
        "# model.load_state_dict(ckpt['model_state'])\n",
        "\n",
        "# df = pd.read_csv(TEST_MANIFEST_CSV)\n",
        "\n",
        "# def _resolve_path(p):\n",
        "#     p = Path(p)\n",
        "#     return p if p.is_absolute() else (ROOT / p).resolve()\n",
        "\n",
        "# df[\"wav_path\"] = df[\"wav_path\"].apply(lambda p: str(_resolve_path(p)))\n",
        "# df[\"txt_path\"] = df[\"txt_path\"].apply(lambda p: str(_resolve_path(p)))\n",
        "# all_preds, all_gt, sample_ids = [], [], []\n",
        "\n",
        "# print(f\"Evaluating {len(df)} samples against {len(valid_labels)} classes...\")\n",
        "\n",
        "# for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "#     gt_vec = parse_ground_truth(row['txt_path'], label_to_idx)\n",
        "#     mel = load_and_preprocess(row['wav_path'], audio_cfg)\n",
        "#     probs = get_prediction(model, mel, DEVICE)\n",
        "\n",
        "#     all_preds.append(probs)\n",
        "#     all_gt.append(gt_vec)\n",
        "#     sample_ids.append(Path(row['wav_path']).stem)\n",
        "\n",
        "\n",
        "# preds_arr = np.array(all_preds)\n",
        "# gts_arr = np.array(all_gt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
